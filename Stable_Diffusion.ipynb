{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zDpCHib5o48i"
   },
   "source": [
    "---\n",
    "# **Start Here**\n",
    "This assumes you have the model in your directory and it is named \"model.ckpt\"\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FxV4KJF4ouJW"
   },
   "source": [
    "---\n",
    "# **Text 2 Image**\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FpEY0oYSp47z"
   },
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Dq5ZTTVHzTWI"
   },
   "outputs": [],
   "source": [
    "cd stable-diffusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yqcUa1DSsVFZ"
   },
   "outputs": [],
   "source": [
    "import argparse, os, sys, glob\n",
    "import torch\n",
    "import numpy as np\n",
    "from omegaconf import OmegaConf\n",
    "from PIL import Image\n",
    "from tqdm import tqdm, trange\n",
    "from itertools import islice\n",
    "from einops import rearrange\n",
    "from torchvision.utils import make_grid\n",
    "import time\n",
    "from pytorch_lightning import seed_everything\n",
    "from torch import autocast\n",
    "from contextlib import contextmanager, nullcontext\n",
    "from ldm.util import instantiate_from_config\n",
    "from ldm.models.diffusion.ddim import DDIMSampler\n",
    "from ldm.models.diffusion.plms import PLMSSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hTqhlJslsc-k"
   },
   "outputs": [],
   "source": [
    "def chunk(it, size):\n",
    "    it = iter(it)\n",
    "    return iter(lambda: tuple(islice(it, size)), ())\n",
    "\n",
    "\n",
    "def load_model_from_config(ckpt, verbose=False):\n",
    "    print(f\"Loading model from {ckpt}\")\n",
    "    pl_sd = torch.load(ckpt, map_location=\"cuda:0\")\n",
    "    if \"global_step\" in pl_sd:\n",
    "        print(f\"Global Step: {pl_sd['global_step']}\")\n",
    "    sd = pl_sd[\"state_dict\"]\n",
    "    return sd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5lFjABclyK1m"
   },
   "outputs": [],
   "source": [
    "def generate(opt,prompt,grid):\n",
    "    device = 'cuda'\n",
    "    images = []\n",
    "\n",
    "    batch_size = opt.n_samples\n",
    "    n_rows = opt.n_rows if opt.n_rows > 0 else batch_size\n",
    "\n",
    "    assert prompt is not None\n",
    "    data = [batch_size * [prompt]]\n",
    "\n",
    "    start_code = torch.randn([opt.n_samples, opt.C, opt.H // opt.f, opt.W // opt.f], device=device)\n",
    "\n",
    "    precision_scope = autocast if opt.precision==\"autocast\" else nullcontext\n",
    "    with torch.no_grad():\n",
    "        all_samples = list()\n",
    "        for n in trange(opt.n_iter, desc=\"Sampling\"):\n",
    "            for prompts in tqdm(data, desc=\"data\"):\n",
    "                with precision_scope(\"cuda\"):\n",
    "                    #modelCS.to(device)\n",
    "                    uc = None\n",
    "                    if opt.scale != 1.0:\n",
    "                        uc = modelCS.get_learned_conditioning(batch_size * [\"\"])\n",
    "                    if isinstance(prompts, tuple):\n",
    "                        prompts = list(prompts)\n",
    "\n",
    "                    c = modelCS.get_learned_conditioning(prompts)\n",
    "                    shape = [opt.C, opt.H // opt.f, opt.W // opt.f]\n",
    "                    #mem = torch.cuda.memory_allocated()/1e6\n",
    "                    #modelCS.to(\"cpu\")\n",
    "                    #while(torch.cuda.memory_allocated()/1e6 >= mem):\n",
    "                    #    time.sleep(1)\n",
    "\n",
    "                    samples_ddim = model.sample(S=opt.ddim_steps,\n",
    "                                                conditioning=c,\n",
    "                                                batch_size=opt.n_samples,\n",
    "                                                shape=shape,\n",
    "                                                verbose=False,\n",
    "                                                unconditional_guidance_scale=opt.scale,\n",
    "                                                unconditional_conditioning=uc,\n",
    "                                                eta=opt.ddim_eta,\n",
    "                                                x_T=start_code)\n",
    "\n",
    "                    #modelFS.to(device)\n",
    "                    for i in range(batch_size):\n",
    "                        x_samples_ddim = modelFS.decode_first_stage(samples_ddim[i].unsqueeze(0))\n",
    "                        x_sample = torch.clamp((x_samples_ddim + 1.0) / 2.0, min=0.0, max=1.0)\n",
    "\n",
    "                        x_sample = 255. * rearrange(x_sample[0].cpu().numpy(), 'c h w -> h w c')\n",
    "                        images +=[Image.fromarray(x_sample.astype(np.uint8))]\n",
    "                    #mem = torch.cuda.memory_allocated()/1e6\n",
    "                    #modelFS.to(\"cpu\")\n",
    "                    #while(torch.cuda.memory_allocated()/1e6 >= mem):\n",
    "                    #    time.sleep(1)\n",
    "\n",
    "                    if grid:\n",
    "                        all_samples.append(x_samples_ddim)\n",
    "\n",
    "                    del samples_ddim\n",
    "                    print(\"memory_final = \", torch.cuda.memory_allocated()/1e6)\n",
    "            if grid:\n",
    "                grid = torch.stack(all_samples, 0)\n",
    "                grid = rearrange(grid, 'n b c h w -> (n b) c h w')\n",
    "                grid = make_grid(grid, nrow=n_rows)\n",
    "                grid = 255. * rearrange(grid, 'c h w -> h w c').cpu().numpy()\n",
    "                images = [Image.fromarray(grid.astype(np.uint8))] + images\n",
    "\n",
    "            print(f'Finished!')\n",
    "            return images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qrAkSMlHqFLd"
   },
   "source": [
    "## 2. Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yUupXuBtsnQh"
   },
   "outputs": [],
   "source": [
    "ckpt = '../model.ckpt' # this points to the model that is in your root gdrive folder\n",
    "\n",
    "sd = load_model_from_config(f\"{ckpt}\")\n",
    "li = []\n",
    "lo = []\n",
    "for key, value in sd.items():\n",
    "    sp = key.split('.')\n",
    "    if(sp[0]) == 'model':\n",
    "        if('input_blocks' in sp):\n",
    "            li.append(key)\n",
    "        elif('middle_block' in sp):\n",
    "            li.append(key)\n",
    "        elif('time_embed' in sp):\n",
    "            li.append(key)\n",
    "        else:\n",
    "            lo.append(key)\n",
    "for key in li:\n",
    "    sd['model1.' + key[6:]] = sd.pop(key)\n",
    "for key in lo:\n",
    "    sd['model2.' + key[6:]] = sd.pop(key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-UDhfoj_p_Mv"
   },
   "source": [
    "## 3. Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9bw5t11msgza"
   },
   "outputs": [],
   "source": [
    "class config():\n",
    "      def __init__(self):\n",
    "        self.config = 'optimizedSD/v1-inference.yaml' # Don't change this\n",
    "        self.ckpt = ckpt # If you want to change the model location, change it on the Load movel section\n",
    "\n",
    "        self.precision = 'autocast' # Change to full and fuck your RAM\n",
    "        self.ddim_eta = 0.0 # Does nothing, keep as is\n",
    "        self.C = 4 # Keep as is\n",
    "\n",
    "        self.seed = 435455\n",
    "\n",
    "        self.ddim_steps = 30 # Keep within 30 ~ 250, higher is better but slower\n",
    "        self.H = 256 # Height, the vertical resolution\n",
    "        self.W = 256 # Width, the horizontal resolution\n",
    "        self.f = 8 # Visual scale maybe, 256x256 with f = 4 seems to use same RAM as 512x512 with f = 8\n",
    "        self.scale = 7.5 # Keep within 4 ~ 25, maybe, changes how the prompt is interpreted\n",
    "\n",
    "        self.n_iter = 1 # Maybe improves it, reccomended to keep as is as it multiplies the waiting time\n",
    "        self.n_samples = 9 # Amount of images outputted\n",
    "        self.n_rows = 3 # How many images per row (used on grid)\n",
    "\n",
    "opt = config()\n",
    "seed_everything(opt.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3jVS88k9mS1w"
   },
   "outputs": [],
   "source": [
    "config = OmegaConf.load(f\"{opt.config}\")\n",
    "config.modelUNet.params.ddim_steps = opt.ddim_steps\n",
    "\n",
    "model = instantiate_from_config(config.modelUNet)\n",
    "_, _ = model.load_state_dict(sd, strict=False)\n",
    "model.eval()\n",
    "    \n",
    "modelCS = instantiate_from_config(config.modelCondStage)\n",
    "_, _ = modelCS.load_state_dict(sd, strict=False)\n",
    "modelCS.eval()\n",
    "modelCS.to('cuda')\n",
    "\n",
    "modelFS = instantiate_from_config(config.modelFirstStage)\n",
    "_, _ = modelFS.load_state_dict(sd, strict=False)\n",
    "modelFS.eval()\n",
    "modelFS.eval()\n",
    "modelFS.to('cuda')\n",
    "\n",
    "if opt.precision == \"autocast\":\n",
    "    model.half()\n",
    "    modelCS.half()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hpTVwndLqQ4X"
   },
   "source": [
    "## 4. Run prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "GdOYu4eWzn9y"
   },
   "outputs": [],
   "source": [
    "prompt = \"Anatomical cross section of a tree with a boney skeletal structure 1970s scientific diagram\" #@param {type:\"string\"}\n",
    "scale = 7.5 #@param {type:\"number\"}\n",
    "height = 512 #@param {type:\"integer\"}\n",
    "width = 512 #@param {type:\"integer\"}\n",
    "n_steps = 30 #@param {type:\"slider\", min:30, max:250, step:5}\n",
    "n_images = 4 #@param {type:\"integer\"}\n",
    "n_rows = 2 #@param {type:\"integer\"}\n",
    "grid = \"yes\" #@param [\"yes\", \"no\"]\n",
    "opt.scale = scale\n",
    "opt.H = height\n",
    "opt.W = width\n",
    "opt.n_samples = n_images\n",
    "opt.n_rows = n_rows\n",
    "opt.ddim_steps = n_steps\n",
    "images = generate(opt=opt, prompt=prompt, grid=(grid==\"yes\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-e8MY5SoqX3u"
   },
   "source": [
    "## Output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5i_5610OPi3x"
   },
   "source": [
    "if you generated more than 9 images(the grid counts too), just add more code lines continuing the sequence\n",
    "\n",
    "If you chose grid, it's the first one\n",
    "\n",
    "Right click and save to download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UavUwJGQzslr"
   },
   "outputs": [],
   "source": [
    "images[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PxZCEfDm2TVr"
   },
   "outputs": [],
   "source": [
    "images[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oz9WfK-Z2Tnr"
   },
   "outputs": [],
   "source": [
    "images[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_xoSKGXdutaE"
   },
   "outputs": [],
   "source": [
    "images[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UAgFGdzHveXn"
   },
   "outputs": [],
   "source": [
    "images[4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QVLTsPxLpAHD"
   },
   "source": [
    "---\n",
    "# **Image 2 Image**\n",
    "---\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_Re8zckxD4O2"
   },
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CXrPmEEowI_S"
   },
   "outputs": [],
   "source": [
    "#cd /content/gdrive/MyDrive/stable-diffusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oaiJGYY7pDcy"
   },
   "outputs": [],
   "source": [
    "import argparse, os, sys, glob\n",
    "import PIL\n",
    "import time\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from omegaconf import OmegaConf\n",
    "from PIL import Image\n",
    "from tqdm import tqdm, trange\n",
    "from itertools import islice\n",
    "from einops import rearrange, repeat\n",
    "from torchvision.utils import make_grid\n",
    "from torch import autocast\n",
    "from contextlib import nullcontext\n",
    "from pytorch_lightning import seed_everything\n",
    "\n",
    "from ldm.util import instantiate_from_config\n",
    "from ldm.models.diffusion.ddim import DDIMSampler\n",
    "from ldm.models.diffusion.plms import PLMSSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_zpHIRmavzkQ"
   },
   "outputs": [],
   "source": [
    "def chunk(it, size):\n",
    "    it = iter(it)\n",
    "    return iter(lambda: tuple(islice(it, size)), ())\n",
    "\n",
    "def load_model_from_config(config, ckpt, verbose=False):\n",
    "    print(f\"Loading model from {ckpt}\")\n",
    "    pl_sd = torch.load(ckpt, map_location=\"cuda:0\")\n",
    "    if \"global_step\" in pl_sd:\n",
    "        print(f\"Global Step: {pl_sd['global_step']}\")\n",
    "    sd = pl_sd[\"state_dict\"]\n",
    "    model = instantiate_from_config(config.model)\n",
    "    m, u = model.load_state_dict(sd, strict=False)\n",
    "    if len(m) > 0 and verbose:\n",
    "        print(\"missing keys:\")\n",
    "        print(m)\n",
    "    if len(u) > 0 and verbose:\n",
    "        print(\"unexpected keys:\")\n",
    "        print(u)\n",
    "\n",
    "    model.cuda()\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "def load_img(path):\n",
    "    image = Image.open(path).convert(\"RGB\")\n",
    "    w, h = image.size\n",
    "    print(f\"loaded input image of size ({w}, {h}) from {path}\")\n",
    "    w, h = map(lambda x: x - x % 32, (w, h))  # resize to integer multiple of 32\n",
    "    image = image.resize((w, h), resample=PIL.Image.LANCZOS)\n",
    "    image = np.array(image).astype(np.float32) / 255.0\n",
    "    image = image[None].transpose(0, 3, 1, 2)\n",
    "    image = torch.from_numpy(image).half()\n",
    "    return 2.*image - 1.\n",
    "\n",
    "def generate(opt,init_img,grid,prompt):\n",
    "    device = 'cuda'\n",
    "    images = []\n",
    "    all_samples = list()\n",
    "\n",
    "    batch_size = opt.n_samples\n",
    "    n_rows = opt.n_rows if opt.n_rows > 0 else batch_size\n",
    "\n",
    "    assert prompt is not None\n",
    "    data = [batch_size * [prompt]]\n",
    "\n",
    "    assert os.path.isfile(init_img)\n",
    "    init_image = load_img(init_img).to(device)\n",
    "    init_image = repeat(init_image, '1 ... -> b ...', b=batch_size)\n",
    "    init_latent = model.get_first_stage_encoding(model.encode_first_stage(init_image))  # move to latent space\n",
    "\n",
    "    sampler.make_schedule(ddim_num_steps=opt.ddim_steps, ddim_eta=opt.ddim_eta, verbose=False)\n",
    "\n",
    "    assert 0. <= opt.strength <= 1., 'can only work with strength in [0.0, 1.0]'\n",
    "    t_enc = int(opt.strength * opt.ddim_steps)\n",
    "    print(f\"target t_enc is {t_enc} steps\")\n",
    "\n",
    "    precision_scope = autocast if opt.precision == \"autocast\" else nullcontext\n",
    "    with torch.no_grad():\n",
    "        with precision_scope(\"cuda\"):\n",
    "            with model.ema_scope():\n",
    "                for n in trange(opt.n_iter, desc=\"Sampling\"):\n",
    "                    for prompts in tqdm(data, desc=\"data\"):\n",
    "                        uc = None\n",
    "                        if opt.scale != 1.0:\n",
    "                            uc = model.get_learned_conditioning(batch_size * [\"\"])\n",
    "                        if isinstance(prompts, tuple):\n",
    "                            prompts = list(prompts)\n",
    "                        c = model.get_learned_conditioning(prompts)\n",
    "\n",
    "                        z_enc = sampler.stochastic_encode(init_latent, torch.tensor([t_enc]*batch_size).to(device))\n",
    "                        samples = sampler.decode(z_enc, c, t_enc, unconditional_guidance_scale=opt.scale,\n",
    "                                                                  unconditional_conditioning=uc,)\n",
    "\n",
    "                        if grid:\n",
    "                            all_samples.append(torch.clamp((model.decode_first_stage(samples) + 1.0) / 2.0, min=0.0, max=1.0))\n",
    "\n",
    "                        for i in range(batch_size):\n",
    "                            x_samples_ddim = model.decode_first_stage(samples[i].unsqueeze(0))\n",
    "                            x_sample = torch.clamp((x_samples_ddim + 1.0) / 2.0, min=0.0, max=1.0)\n",
    "\n",
    "                            x_sample = 255. * rearrange(x_sample[0].cpu().numpy(), 'c h w -> h w c')\n",
    "                            images += [Image.fromarray(x_sample.astype(np.uint8))]\n",
    "                        del samples\n",
    "    if grid:\n",
    "        grid = torch.stack(all_samples, 0)\n",
    "        grid = rearrange(grid, 'n b c h w -> (n b) c h w')\n",
    "        grid = make_grid(grid, nrow=n_rows)\n",
    "        grid = 255. * rearrange(grid, 'c h w -> h w c').cpu().numpy()\n",
    "        images = [Image.fromarray(grid.astype(np.uint8))] + images\n",
    "\n",
    "    print(f'Finished!')\n",
    "    return images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z-0JRk2yD1CH"
   },
   "source": [
    "## 2. Settings and loading model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Pdl7SBzEv6C5"
   },
   "outputs": [],
   "source": [
    "class config():\n",
    "      def __init__(self):\n",
    "        self.config = 'configs/stable-diffusion/v1-inference.yaml' # Don't change this\n",
    "        self.ckpt = '../model.ckpt' # this points to the model that is in your root gdrive folder\n",
    "\n",
    "        self.precision = 'autocast' # Change to full and fuck your RAM\n",
    "        self.ddim_eta = 0.0 # Does nothing, keep as is\n",
    "        self.C = 4 # Keep as is\n",
    "\n",
    "        self.seed = 7777\n",
    "\n",
    "        self.ddim_steps = 30 # Keep within 30 ~ 250, higher is better but slower\n",
    "        self.H = 512 # Height, the vertical resolution\n",
    "        self.W = 512 # Width, the horizontal resolution\n",
    "        self.f = 8 # Visual scale maybe, 256x256 with f = 4 seems to use same RAM as 512x512 with f = 8\n",
    "        self.scale = 7.5 # Keep within 4 ~ 25, maybe, changes how the prompt is interpreted\n",
    "        self.strength = 0.7 # How agressive it is, keep between 0.2 ~ 1.0\n",
    "\n",
    "        self.n_iter = 1 # Maybe improves it, reccomended to keep as is as it multiplies the waiting time\n",
    "        self.n_samples = 9 # Amount of images outputted\n",
    "        self.n_rows = 3 # How many images per row (used on grid)\n",
    "\n",
    "opt = config()\n",
    "seed_everything(opt.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4vrgAiwZzrD_"
   },
   "outputs": [],
   "source": [
    "config = OmegaConf.load(f\"{opt.config}\")\n",
    "model = load_model_from_config(config, f\"{opt.ckpt}\").half()\n",
    "sampler = DDIMSampler(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TTJqduQ_EBZU"
   },
   "source": [
    "## 3. Run prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "519jUkb1RRib"
   },
   "source": [
    "add \"../\" before the name for images in the root gdrive folder\n",
    "\n",
    "this video(not mine) shows how to upload images: https://siasky.net/_ABvKVbl9c9FDFywm4HauIr3g2EgPg2YbRE8burq8IsK-w\n",
    "\n",
    "strength sets how agressive it is when modifying the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "j81cBoWg0TI4"
   },
   "outputs": [],
   "source": [
    "prompt = \"beautiful landscape mountain river shot wide-angle national park nature preserve\" #@param {type:\"string\"}\n",
    "img = \"../img.png\" #@param {type:\"string\"}\n",
    "scale = 5 #@param {type:\"number\"}\n",
    "height = 512 #@param {type:\"integer\"}\n",
    "width = 512 #@param {type:\"integer\"}\n",
    "n_steps = 100 #@param {type:\"slider\", min:30, max:250, step:5}\n",
    "strength = 0.51 #@param {type:\"slider\", min:0, max:1, step:0.01}\n",
    "n_images = 4 #@param {type:\"integer\"}\n",
    "n_rows = 2 #@param {type:\"integer\"}\n",
    "grid = \"yes\" #@param [\"yes\", \"no\"]\n",
    "opt.scale = scale\n",
    "opt.H = height\n",
    "opt.W = width\n",
    "opt.n_samples = n_images\n",
    "opt.n_rows = n_rows\n",
    "opt.ddim_steps = n_steps\n",
    "opt.strength = strength\n",
    "images = generate(opt, init_img=img, grid=(grid==\"yes\"), prompt=prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cN-uLLBEEGvU"
   },
   "source": [
    "## Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JZcUw_0ZAHIZ"
   },
   "outputs": [],
   "source": [
    "images[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0JjnsHKQAKh8"
   },
   "outputs": [],
   "source": [
    "images[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g3ep6KrNALgd"
   },
   "outputs": [],
   "source": [
    "images[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OPdsSouaAqdk"
   },
   "outputs": [],
   "source": [
    "images[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9PMCGTfrVpC8"
   },
   "outputs": [],
   "source": [
    "images[4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WY3mrjfBJ4K-"
   },
   "source": [
    "---\n",
    "# **Text 2 Image with k-diffusion**\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jS2j9vRWKGEE"
   },
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fYSOCX0sKLQE"
   },
   "outputs": [],
   "source": [
    "#cd /content/gdrive/MyDrive/stable-diffusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n4k6xC_wKLI5"
   },
   "outputs": [],
   "source": [
    "import argparse, os, sys, glob\n",
    "import time\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from omegaconf import OmegaConf\n",
    "from PIL import Image\n",
    "from tqdm import tqdm, trange\n",
    "from itertools import islice\n",
    "from einops import rearrange\n",
    "from torchvision.utils import make_grid\n",
    "from pytorch_lightning import seed_everything\n",
    "from torch import autocast\n",
    "from contextlib import contextmanager, nullcontext\n",
    "\n",
    "import accelerate\n",
    "import k_diffusion as K\n",
    "import torch.nn as nn\n",
    "\n",
    "from ldm.util import instantiate_from_config\n",
    "from ldm.models.diffusion.ddim import DDIMSampler\n",
    "from ldm.models.diffusion.plms import PLMSSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hRuexkybKOqo"
   },
   "outputs": [],
   "source": [
    "def chunk(it, size):\n",
    "    it = iter(it)\n",
    "    return iter(lambda: tuple(islice(it, size)), ())\n",
    "\n",
    "\n",
    "def load_model_from_config(ckpt, verbose=False):\n",
    "    print(f\"Loading model from {ckpt}\")\n",
    "    pl_sd = torch.load(ckpt, map_location=\"cuda:0\")\n",
    "    if \"global_step\" in pl_sd:\n",
    "        print(f\"Global Step: {pl_sd['global_step']}\")\n",
    "    sd = pl_sd[\"state_dict\"]\n",
    "    return sd\n",
    "\n",
    "class CFGDenoiser(nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.inner_model = model\n",
    "\n",
    "    def forward(self, x, sigma, uncond, cond, cond_scale):\n",
    "        x_in = torch.cat([x] * 2)\n",
    "        sigma_in = torch.cat([sigma] * 2)\n",
    "        cond_in = torch.cat([uncond, cond])\n",
    "        uncond, cond = self.inner_model(x_in, sigma_in, cond=cond_in).chunk(2)\n",
    "        return uncond + (cond - uncond) * cond_scale\n",
    "\n",
    "def generate(opt,prompt,grid):\n",
    "    accelerator = accelerate.Accelerator()\n",
    "    device = accelerator.device\n",
    "    images = []\n",
    "\n",
    "    seeds = torch.randint(-2 ** 63, 2 ** 63 - 1, [accelerator.num_processes])\n",
    "    torch.manual_seed(seeds[accelerator.process_index].item())\n",
    "\n",
    "    batch_size = opt.n_samples\n",
    "    n_rows = opt.n_rows if opt.n_rows > 0 else batch_size\n",
    "\n",
    "    assert prompt is not None\n",
    "    data = [batch_size * [prompt]]\n",
    "\n",
    "    start_code = torch.randn([opt.n_samples, opt.C, opt.H // opt.f, opt.W // opt.f], device=device)\n",
    "\n",
    "    precision_scope = autocast if opt.precision==\"autocast\" else nullcontext\n",
    "    with torch.no_grad():\n",
    "        all_samples = list()\n",
    "        with precision_scope(\"cuda\"):\n",
    "            for n in trange(opt.n_iter, desc=\"Sampling\", disable =not accelerator.is_main_process):\n",
    "                for prompts in tqdm(data, desc=\"data\", disable =not accelerator.is_main_process):\n",
    "                    #modelCS.to(device)\n",
    "                    uc = None\n",
    "                    if opt.scale != 1.0:\n",
    "                        uc = modelCS.get_learned_conditioning(batch_size * [\"\"])\n",
    "                    if isinstance(prompts, tuple):\n",
    "                        prompts = list(prompts)\n",
    "\n",
    "                    c = modelCS.get_learned_conditioning(prompts)\n",
    "                    shape = [opt.C, opt.H // opt.f, opt.W // opt.f]\n",
    "                    #mem = torch.cuda.memory_allocated()/1e6\n",
    "                    #modelCS.to(\"cpu\")\n",
    "                    #while(torch.cuda.memory_allocated()/1e6 >= mem):\n",
    "                    #    time.sleep(1)\n",
    "\n",
    "                    sigmas = model_wrap.get_sigmas(opt.ddim_steps)\n",
    "                    torch.manual_seed(opt.seed)\n",
    "                    x = torch.randn([opt.n_samples, *shape], device=device) * sigmas[0] # for GPU draw\n",
    "                    model_wrap_cfg = CFGDenoiser(model_wrap)\n",
    "                    extra_args = {'cond': c, 'uncond': uc, 'cond_scale': opt.scale}\n",
    "                    samples_ddim = K.sampling.sample_lms(model_wrap_cfg, x, sigmas, extra_args=extra_args, disable=not accelerator.is_main_process)\n",
    "                    \n",
    "                    #modelFS.to(device)\n",
    "                    for i in range(batch_size):\n",
    "                        x_samples_ddim = modelFS.decode_first_stage(samples_ddim[i].unsqueeze(0))\n",
    "                        x_samples_ddim = torch.clamp((x_samples_ddim + 1.0) / 2.0, min=0.0, max=1.0)\n",
    "                        x_sample = accelerator.gather(x_samples_ddim)\n",
    "                        if grid:\n",
    "                            all_samples.append(x_sample)\n",
    "                        x_sample = 255. * rearrange(x_sample[0].cpu().numpy(), 'c h w -> h w c')\n",
    "                        images +=[Image.fromarray(x_sample.astype(np.uint8))]\n",
    "                    #mem = torch.cuda.memory_allocated()/1e6\n",
    "                    #modelFS.to(\"cpu\")\n",
    "                    #while(torch.cuda.memory_allocated()/1e6 >= mem):\n",
    "                    #    time.sleep(1)\n",
    "\n",
    "                    del samples_ddim\n",
    "            if grid:\n",
    "                grid = torch.stack(all_samples, 0)\n",
    "                grid = rearrange(grid, 'n b c h w -> (n b) c h w')\n",
    "                grid = make_grid(grid, nrow=n_rows)\n",
    "                grid = 255. * rearrange(grid, 'c h w -> h w c').cpu().numpy()\n",
    "                images = [Image.fromarray(grid.astype(np.uint8))] + images\n",
    "\n",
    "            print(f'Finished!')\n",
    "            return images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "an2-gonRKHPf"
   },
   "source": [
    "## 2. Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9U9NTLPDKuxL"
   },
   "outputs": [],
   "source": [
    "ckpt = '../model.ckpt' # this points to the model that is in your root gdrive folder\n",
    "\n",
    "sd = load_model_from_config(f\"{ckpt}\")\n",
    "li = []\n",
    "lo = []\n",
    "for key, value in sd.items():\n",
    "    sp = key.split('.')\n",
    "    if(sp[0]) == 'model':\n",
    "        if('input_blocks' in sp):\n",
    "            li.append(key)\n",
    "        elif('middle_block' in sp):\n",
    "            li.append(key)\n",
    "        elif('time_embed' in sp):\n",
    "            li.append(key)\n",
    "        else:\n",
    "            lo.append(key)\n",
    "for key in li:\n",
    "    sd['model1.' + key[6:]] = sd.pop(key)\n",
    "for key in lo:\n",
    "    sd['model2.' + key[6:]] = sd.pop(key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wzupc6zkKF5y"
   },
   "source": [
    "## 3. Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S5PI2jYtKwov"
   },
   "outputs": [],
   "source": [
    "class config():\n",
    "      def __init__(self):\n",
    "        self.config = 'optimizedSD/v1-inference.yaml' # Don't change this\n",
    "        self.ckpt = ckpt # If you want to change the model location, change it on the Load movel section\n",
    "\n",
    "        self.precision = 'autocast' # Change to full and fuck your RAM\n",
    "        self.ddim_eta = 0.0 # Does nothing, keep as is\n",
    "        self.C = 4 # Keep as is\n",
    "\n",
    "        self.seed = 435455\n",
    "\n",
    "        self.ddim_steps = 30 # Keep within 30 ~ 250, higher is better but slower\n",
    "        self.H = 256 # Height, the vertical resolution\n",
    "        self.W = 256 # Width, the horizontal resolution\n",
    "        self.f = 8 # Visual scale maybe, 256x256 with f = 4 seems to use same RAM as 512x512 with f = 8\n",
    "        self.scale = 7.5 # Keep within 4 ~ 25, maybe, changes how the prompt is interpreted\n",
    "\n",
    "        self.n_iter = 1 # Maybe improves it, reccomended to keep as is as it multiplies the waiting time\n",
    "        self.n_samples = 4 # Amount of images outputted\n",
    "        self.n_rows = 2 # How many images per row (used on grid)\n",
    "\n",
    "opt = config()\n",
    "seed_everything(opt.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DvF_YZUyKwh5"
   },
   "outputs": [],
   "source": [
    "config = OmegaConf.load(f\"{opt.config}\")\n",
    "config.modelUNet.params.ddim_steps = opt.ddim_steps\n",
    "\n",
    "model = instantiate_from_config(config.modelUNet)\n",
    "_, _ = model.load_state_dict(sd, strict=False)\n",
    "model.eval()\n",
    "    \n",
    "modelCS = instantiate_from_config(config.modelCondStage)\n",
    "_, _ = modelCS.load_state_dict(sd, strict=False)\n",
    "modelCS.eval()\n",
    "modelCS.to('cuda')\n",
    "\n",
    "modelFS = instantiate_from_config(config.modelFirstStage)\n",
    "_, _ = modelFS.load_state_dict(sd, strict=False)\n",
    "modelFS.eval()\n",
    "modelFS.to('cuda')\n",
    "\n",
    "if opt.precision == \"autocast\":\n",
    "    model.half()\n",
    "    modelCS.half()\n",
    "\n",
    "model_wrap = K.external.CompVisDenoiser(model)\n",
    "sigma_min, sigma_max = model_wrap.sigmas[0].item(), model_wrap.sigmas[-1].item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RSpmqvb0KF1M"
   },
   "source": [
    "## 4. Run Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "0_kIZqhiK0mO"
   },
   "outputs": [],
   "source": [
    "prompt = \"Anatomical cross section of a tree with a boney skeletal structure 1970s scientific diagram\" #@param {type:\"string\"}\n",
    "scale = 7.5 #@param {type:\"number\"}\n",
    "height = 512 #@param {type:\"integer\"}\n",
    "width = 512 #@param {type:\"integer\"}\n",
    "n_steps = 30 #@param {type:\"slider\", min:30, max:250, step:5}\n",
    "n_images = 4 #@param {type:\"integer\"}\n",
    "n_rows = 2 #@param {type:\"integer\"}\n",
    "grid = \"yes\" #@param [\"yes\", \"no\"]\n",
    "opt.scale = scale\n",
    "opt.H = height\n",
    "opt.W = width\n",
    "opt.n_samples = n_images\n",
    "opt.n_rows = n_rows\n",
    "opt.ddim_steps = n_steps\n",
    "images = generate(opt=opt, prompt=prompt, grid=(grid==\"yes\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aQObuZkFKFsB"
   },
   "source": [
    "## Output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lp-qQ08IK2Oh"
   },
   "source": [
    "If you generated more than 5 images(the grid counts too), just add more code lines continuing the sequence\n",
    "\n",
    "If you chose grid, it's the first one\n",
    "\n",
    "Right click and save to download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aUB8qNcRK1kn"
   },
   "outputs": [],
   "source": [
    "images[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yuD5K0YQK1eZ"
   },
   "outputs": [],
   "source": [
    "images[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6nb-QWw2K1Yc"
   },
   "outputs": [],
   "source": [
    "images[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3BmTtPnLK1TO"
   },
   "outputs": [],
   "source": [
    "images[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TVPEkBb7K1PA"
   },
   "outputs": [],
   "source": [
    "images[4]"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "zDpCHib5o48i",
    "FESdr0SFq6yM",
    "bHw1XtAsrF2j",
    "FxV4KJF4ouJW",
    "FpEY0oYSp47z",
    "qrAkSMlHqFLd",
    "-UDhfoj_p_Mv",
    "hpTVwndLqQ4X",
    "-e8MY5SoqX3u",
    "QVLTsPxLpAHD",
    "_Re8zckxD4O2",
    "Z-0JRk2yD1CH",
    "TTJqduQ_EBZU",
    "cN-uLLBEEGvU",
    "WY3mrjfBJ4K-",
    "jS2j9vRWKGEE",
    "an2-gonRKHPf",
    "Wzupc6zkKF5y",
    "RSpmqvb0KF1M",
    "aQObuZkFKFsB"
   ],
   "name": "Stable-Diffusion on Colab",
   "private_outputs": true,
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
