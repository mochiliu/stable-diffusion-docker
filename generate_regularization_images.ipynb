{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "280d40c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse, os, sys, glob\n",
    "\n",
    "sys.path.extend([\n",
    "    '/workspace/Dreambooth-SD-optimized',\n",
    "])\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from omegaconf import OmegaConf\n",
    "from PIL import Image\n",
    "from tqdm import tqdm, trange\n",
    "from itertools import islice\n",
    "from einops import rearrange\n",
    "from torchvision.utils import make_grid, save_image\n",
    "import time\n",
    "from pytorch_lightning import seed_everything\n",
    "from torch import autocast\n",
    "from contextlib import contextmanager, nullcontext\n",
    "\n",
    "from ldm.util import instantiate_from_config\n",
    "from ldm.models.diffusion.ddim import DDIMSampler\n",
    "from ldm.models.diffusion.plms import PLMSSampler\n",
    "\n",
    "\n",
    "def chunk(it, size):\n",
    "    it = iter(it)\n",
    "    return iter(lambda: tuple(islice(it, size)), ())\n",
    "\n",
    "\n",
    "def load_model_from_config(config, ckpt, verbose=False):\n",
    "    print(f\"Loading model from {ckpt}\")\n",
    "    pl_sd = torch.load(ckpt, map_location=\"cpu\")\n",
    "    if \"global_step\" in pl_sd:\n",
    "        print(f\"Global Step: {pl_sd['global_step']}\")\n",
    "    sd = pl_sd[\"state_dict\"]\n",
    "    model = instantiate_from_config(config.model)\n",
    "    m, u = model.load_state_dict(sd, strict=False)\n",
    "    if len(m) > 0 and verbose:\n",
    "        print(\"missing keys:\")\n",
    "        print(m)\n",
    "    if len(u) > 0 and verbose:\n",
    "        print(\"unexpected keys:\")\n",
    "        print(u)\n",
    "\n",
    "    model.cuda()\n",
    "    model.eval()\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec034510",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from /workspace/mnt/models/sd-v1-4.ckpt\n",
      "Global Step: 470000\n",
      "LatentDiffusion: Running in eps-prediction mode\n"
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "opt = parser.parse_args(args=[])\n",
    "opt.prompt = 'a photo of a dog'\n",
    "opt.precision='autocast'\n",
    "opt.seed=42\n",
    "opt.ckpt='/workspace/mnt/models/sd-v1-4.ckpt'\n",
    "opt.config='/workspace/Dreambooth-SD-optimized/configs/stable-diffusion/v1-inference.yaml'\n",
    "opt.scale=10.\n",
    "opt.n_rows=0\n",
    "opt.n_samples=8\n",
    "opt.f=8\n",
    "opt.C=4\n",
    "opt.W=512\n",
    "opt.H=512\n",
    "opt.n_iter=25\n",
    "opt.ddim_eta=0.0\n",
    "opt.fixed_code=False\n",
    "opt.laion400m=False\n",
    "opt.plms=False\n",
    "opt.ddim_steps=50\n",
    "opt.skip_save=False\n",
    "opt.skip_grid=True\n",
    "opt.outdir='/workspace/mnt/outputs/regularization_images/'\n",
    "opt.from_file=False\n",
    "\n",
    "if opt.laion400m:\n",
    "    print(\"Falling back to LAION 400M model...\")\n",
    "    opt.config = \"configs/latent-diffusion/txt2img-1p4B-eval.yaml\"\n",
    "    opt.ckpt = \"models/ldm/text2img-large/model.ckpt\"\n",
    "    opt.outdir = \"outputs/txt2img-samples-laion400m\"\n",
    "\n",
    "seed_everything(opt.seed)\n",
    "\n",
    "config = OmegaConf.load(f\"{opt.config}\")\n",
    "model = load_model_from_config(config, f\"{opt.ckpt}\")\n",
    "#model.embedding_manager.load(opt.embedding_path)\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "if opt.plms:\n",
    "    sampler = PLMSSampler(model)\n",
    "else:\n",
    "    sampler = DDIMSampler(model)\n",
    "\n",
    "os.makedirs(opt.outdir, exist_ok=True)\n",
    "outpath = opt.outdir\n",
    "\n",
    "batch_size = opt.n_samples\n",
    "n_rows = opt.n_rows if opt.n_rows > 0 else batch_size\n",
    "if not opt.from_file:\n",
    "    prompt = opt.prompt\n",
    "    assert prompt is not None\n",
    "    data = [batch_size * [prompt]]\n",
    "\n",
    "else:\n",
    "    print(f\"reading prompts from {opt.from_file}\")\n",
    "    with open(opt.from_file, \"r\") as f:\n",
    "        data = f.read().splitlines()\n",
    "        data = list(chunk(data, batch_size))\n",
    "\n",
    "sample_path = os.path.join(outpath, \"samples\")\n",
    "os.makedirs(sample_path, exist_ok=True)\n",
    "base_count = len(os.listdir(sample_path))\n",
    "grid_count = len(os.listdir(outpath)) - 1\n",
    "\n",
    "start_code = None\n",
    "if opt.fixed_code:\n",
    "    start_code = torch.randn([opt.n_samples, opt.C, opt.H // opt.f, opt.W // opt.f], device=device)\n",
    "\n",
    "precision_scope = autocast if opt.precision==\"autocast\" else nullcontext\n",
    "with torch.no_grad():\n",
    "    with precision_scope(\"cuda\"):\n",
    "        with model.ema_scope():\n",
    "            tic = time.time()\n",
    "            all_samples = list()\n",
    "            for n in trange(opt.n_iter, desc=\"Sampling\"):\n",
    "                opt.seed+=1\n",
    "                for prompts in tqdm(data, desc=\"data\"):\n",
    "                    uc = None\n",
    "                    if opt.scale != 1.0:\n",
    "                        uc = model.get_learned_conditioning(batch_size * [\"\"])\n",
    "                    if isinstance(prompts, tuple):\n",
    "                        prompts = list(prompts)\n",
    "                    c = model.get_learned_conditioning(prompts)\n",
    "                    shape = [opt.C, opt.H // opt.f, opt.W // opt.f]\n",
    "                    samples_ddim, _ = sampler.sample(S=opt.ddim_steps,\n",
    "                                                     conditioning=c,\n",
    "                                                     batch_size=opt.n_samples,\n",
    "                                                     shape=shape,\n",
    "                                                     verbose=False,\n",
    "                                                     unconditional_guidance_scale=opt.scale,\n",
    "                                                     unconditional_conditioning=uc,\n",
    "                                                     eta=opt.ddim_eta,\n",
    "                                                     x_T=start_code)\n",
    "\n",
    "                    x_samples_ddim = model.decode_first_stage(samples_ddim)\n",
    "                    x_samples_ddim = torch.clamp((x_samples_ddim + 1.0) / 2.0, min=0.0, max=1.0)\n",
    "\n",
    "                    if not opt.skip_save:\n",
    "                        for x_sample in x_samples_ddim:\n",
    "                            x_sample = 255. * rearrange(x_sample.cpu().numpy(), 'c h w -> h w c')\n",
    "                            Image.fromarray(x_sample.astype(np.uint8)).save(\n",
    "                                os.path.join(sample_path, f\"{base_count:05}.jpg\"))\n",
    "                            base_count += 1\n",
    "\n",
    "                    if not opt.skip_grid:\n",
    "                        all_samples.append(x_samples_ddim)\n",
    "\n",
    "            if not opt.skip_grid:\n",
    "                # additionally, save as grid\n",
    "                grid = torch.stack(all_samples, 0)\n",
    "                grid = rearrange(grid, 'n b c h w -> (n b) c h w')\n",
    "\n",
    "                for i in range(grid.size(0)):\n",
    "                    save_image(grid[i, :, :, :], os.path.join(outpath,opt.prompt+'_{}.png'.format(i)))\n",
    "                grid = make_grid(grid, nrow=n_rows)\n",
    "\n",
    "                # to image\n",
    "                grid = 255. * rearrange(grid, 'c h w -> h w c').cpu().numpy()\n",
    "                Image.fromarray(grid.astype(np.uint8)).save(os.path.join(outpath, f'{prompt.replace(\" \", \"-\")}-{grid_count:04}.jpg'))\n",
    "                grid_count += 1\n",
    "\n",
    "\n",
    "\n",
    "            toc = time.time()\n",
    "\n",
    "print(f\"Your samples are ready and waiting for you here: \\n{outpath} \\n\"\n",
    "      f\" \\nEnjoy.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "464814c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e72171",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
